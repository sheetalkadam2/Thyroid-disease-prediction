# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pl2yU7wekSCu87VbgmxArUSAi_YC6pfK
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns

file = open("/content/thyroid.csv")
df = pd.read_csv(file)

df

df = df.drop(['other'], axis=1)



feature_cols = ["age",
                "sex",
                "on_thyroxine",
                "query_on_thyroxine",
                "on_antithyroid_medication",
                "sick",
                "pregnant",
                "thyroid_surgery",
                "I131_treatment",
                "query_hypothyroid",
                "query_hyperthyroid",
                "lithium",
                "goitre",
                "tumor",
                "hypopituitary",
                "psych",
                "TSH measured",
                "TSH",
                "T3_measured",
                "T3",
                "TT4_measured",
                "TT4",
                "T4U_measured",
                "T4U",
                "FTI_measured",
                "FTI",
                "TBG_measured",
                "TBG",
               "target"]

df.columns = feature_cols

df

df = df.drop("T4U", axis=1)

df

df = df.drop("T4U_measured", axis=1)

df

"""Splitting target
Now we can check that the target columns as many categorial names with some indicate numbers so going to split with the respective features
thyroid.names file content
The diagnosis consists of a string of letters indicating diagnosed conditions. A diagnosis "-" indicates no condition requiring comment. A diagnosis of the form "X|Y" is interpreted as "consistent with X, but more likely Y". The conditions are divided into groups where each group corresponds to a class of comments.

	Letter	Diagnosis
	------	---------

hyperthyroid conditions:

	A	hyperthyroid
	B	T3 toxic
	C	toxic goitre
	D	secondary toxic

hypothyroid conditions:

	E	hypothyroid
	F	primary hypothyroid
	G	compensated hypothyroid
	H	secondary hypothyroid

binding protein:

	I	increased binding protein
	J	decreased binding protein

general health:

	K	concurrent non-thyroidal illness

replacement therapy:

	L	consistent with replacement therapy
	M	underreplaced
	N	overreplaced

antithyroid treatment:

	O	antithyroid drugs
	P	I131 treatment
	Q	surgery

miscellaneous:

	R	discordant assay results
	S	elevated TBG
	T	elevated thyroid hormones

In experiments with an earlier version of this archive, decision trees were derived for the most frequent classes of comments, namely

hyperthyroid conditions (A, B, C, D)
hypothyroid conditions (E, F, G, H)
binding protein (I, J)
general health (K)
replacement therapy (L, M, N)
discordant results (R)
"""

target = df.target
create = target.str.split('([A-Za-z]+)', expand=True)
create = create[1]
target = create.replace({None:'Z'}) #here z is none type
df.target = target
df.target.unique()

df

"""Now we want to impute the null values but this case the null values are marked as in '?' so we can do some tricks"""

df = df.replace(['?'],np.nan)

df.isnull().sum()

# here we can see the TBG has more null observations it will tremendously occur problem so we can remove and some of the other 
# feautre rows which is not useful

df.drop(['TBG_measured','TBG','T3_measured','TSH measured','TT4_measured','FTI_measured'],axis=1,inplace=True)

df

df.isnull().sum()

df.sex.replace({'F':2,'M':1},inplace=True)

round_Values = round(df.sex.mean())
df.sex.fillna(round_Values,inplace=True)

df.sex.unique()

df.isnull().sum()

# now we will impute the null values with knn imputer
from sklearn.impute import KNNImputer
knnimp = KNNImputer(n_neighbors=3)

cols = ['TSH','T3','TT4','FTI']
for i in cols:
    df[i] = knnimp.fit_transform(df[[i]])

df.isnull().sum() # now we can see there is no null values

df.info()

"""Exploratory Data Analysis"""

plt.figure(figsize=(20,20))
sns.heatmap(df.corr(),annot=True)

df.corr()

# we can't find corr for all variable because some of the features are in categorial object so we want to do label encoder

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

cols = df.select_dtypes(include=['object'])

for i in cols.columns:
    try:
        df[i] = le.fit_transform(df[i])
    except:
        continue

# now we can see their is correlation in some features
for a in range(len(df.corr())):
    for b in range(a):
        if((df.corr().iloc[a,b]) >= 0.7):
            print(df.corr().columns[b])

# so TT4 has high correlation among all we can remove

df.drop('TT4',axis=1,inplace=True)

df.hist(bins=25,figsize=(20,20));

"""Now we can see the data normally distributed in some features and some are categorigal now we have to normalize the values
because most of the values lies between 0 to 500 in x-axis
"""

# X and Y split

X = df.drop('target',axis=1)
y = df.target
df2 = X # for on-going process without PCA

y.unique() # we can see there is 29 types are present => 29 categorigal values

"""PCA Technique
First we use PCA then see the result then we move to normal modeling(without PCA)
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=10)

v = pca.fit_transform(X)

X_pca = pd.DataFrame(data = v, columns = ['component_1', 'component_2', 'component_3', 'component_4', 'component_5', 'component_6', 'component_7', 'component_8', 'component_9', 'component_10'])
X_pca

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

for i in X_pca.columns:
    X_pca[i] = scaler.fit_transform(X_pca[[i]])

X_pca.hist(bins=25,figsize=(20,20));

X_pca

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_pca,y,test_size=0.33,random_state=2)

"""Model Selection"""

from sklearn.metrics import accuracy_score

"""Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(max_depth=3)
clf = tree.fit(X_train,y_train)
treepredict = clf.predict(X_test)

accuracy_score(treepredict,y_test)

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(max_depth=2,n_estimators=200)
rclf = rf.fit(X_train,y_train)
rfpred = rclf.predict(X_test)
accuracy_score(rfpred,y_test)

"""K-NN Classifier"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
knnclf = neigh.fit(X_train,y_train)
y_pred = knnclf.predict(X_test)

accuracy_score(y_pred,y_test)

"""SVM"""

from sklearn.svm import SVC
svm = SVC(kernel="sigmoid")
sclf = svm.fit(X_train,y_train)
y_pred = sclf.predict(X_test)
accuracy_score(y_pred,y_test)

"""logisitic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=1000)
lrclf = lr.fit(X_train,y_train)
y_pred = lrclf.predict(X_test)
accuracy_score(y_pred,y_test)

"""Now we can get clear with usage of PCA and now we will see without PCA"""

df2

plt.figure(figsize=(20,20))
sns.heatmap(df2.corr(),annot=True)

# there is no so much correlation
# now we can normalize the value
for i in df2.columns:
    print("\n\n")
    print(i)
    print(df2[i].unique())

cols = ['age','sex','TSH','T3','FTI']
for i in cols:
    df2[i] = scaler.fit_transform(df2[[i]])

# splitting X and y values

X = df2
y = df['target']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

"""Model Selection"""

from sklearn.metrics import accuracy_score

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(max_depth=3)
clf = tree.fit(X_train,y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_pred,y_test)

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(max_depth=2,n_estimators=200)
rclf = rf.fit(X_train,y_train)
y_pred = rclf.predict(X_test)
accuracy_score(y_pred,y_test)

"""K-NN Classifier"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
knnclf = neigh.fit(X_train,y_train)
y_pred = knnclf.predict(X_test)
accuracy_score(y_pred,y_test)

"""SVM"""

from sklearn.svm import SVC
svm = SVC(kernel="sigmoid")
sclf = svm.fit(X_train,y_train)
y_pred = sclf.predict(X_test)
accuracy_score(y_pred,y_test)

"""Now we can say that with PCA technique we can use KNN because it provides best result as 84% but without PCA with lot of varaible we can use decision tree to get better result you can get some insights by above methods

Building a Predictive System
"""

!pip install numpy

input_data = (0,0,0,0,0,0,0,0,0,0,0,0,0,0,842,2603,441,808,801)

# change the input data to a numpy array
input_data_as_numpy_array= np.asarray(input_data)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape (1,-1)

prediction = rclf.predict (input_data_reshaped)

print (prediction)

if (prediction[0]-- 0):
  print('The Person does not have a Thyroid Disease')
else:
    print('The Person has Thyroid Disease')

"""Saving trained model

"""

import pickle

filename = 'Thyroid_disease_model.sav'
pickle.dump(rclf, open(filename,'wb'))

#loading the saved model
loaded_model = pickle.load(open('Thyroid_disease_model.sav','rb'))

input_data = (0,0,0,0,0,0,0,0,0,0,0,0,0,0,842,2603,441,808,801)

# change the input data to a numpy array
input_data_as_numpy_array= np.asarray(input_data)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape (1,-1)

prediction = rclf.predict (input_data_reshaped)
print (prediction)
if (prediction[0]-- 0):
  print('The Person does not have a Thyroid Disease')
else:
    print('The Person has Thyroid Disease')